{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn90ivIqFyEa",
    "outputId": "4450a5c6-32a6-4fb3-f010-e8260c8d5742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nz6axWDDNCwq",
    "outputId": "9a4bc7e5-8ee0-456f-80e4-0a67c30ae9cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer #can try using tf-idf to see if improved accuracy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import preprocessing #encode labels as integers from 0-7\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "##\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHESPyMNYljd"
   },
   "source": [
    "# **Data** **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8XV9V1iwV0fQ"
   },
   "outputs": [],
   "source": [
    "#change filepath\n",
    "pd_train_data = pd.read_csv('/content/gdrive/My Drive/ECSE 551 Project 2/train.csv')\n",
    "pd_test_data = pd.read_csv('/content/gdrive/My Drive/ECSE 551 Project 2/test.csv')\n",
    "#pd_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6qNR12glaDm"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fN5qsx8WJR3",
    "outputId": "1f9e0549-c9af-449e-dd52-ff014ece9c1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#uncomment this block to run the logistic regression model\n",
    "'''\n",
    "'''Create Vocabulary'''\n",
    "pf = pd_train_data.drop_duplicates(subset='body')\n",
    "#print(pd_train_data['body'][9745]==pd_train_data['body'][9785])\n",
    "#print(pf.shape)\n",
    "#train_data = pd_train_data['body'].tolist()\n",
    "#train_labels = pd_train_data['subreddit'].tolist()\n",
    "train_data = pf['body'].tolist()\n",
    "train_labels = pf['subreddit'].tolist()\n",
    "sklearn_stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer0 = TfidfVectorizer(min_df=.0004,stop_words=sklearn_stopwords,analyzer='word')  \n",
    "X0 = vectorizer0.fit_transform(train_data)\n",
    "features=vectorizer0.get_feature_names()\n",
    "train_data20 = X0.toarray()\n",
    "train_data0 = SelectKBest(chi2,k=5000).fit(train_data20,train_labels)\n",
    "\n",
    "new_features = []\n",
    "for index in train_data0.get_support(indices=True):\n",
    "        new_features.append(features[index])\n",
    "print(len(new_features))\n",
    "rpg_data=[]\n",
    "rpg_label=[]\n",
    "anime_data=[]\n",
    "datascience_data=[]\n",
    "datascience_label=[]\n",
    "hardware_data=[]\n",
    "hardware_label=[]\n",
    "cars_data=[]\n",
    "gamernews_data=[]\n",
    "gamernews_label=[]\n",
    "gamedev_data=[]\n",
    "gamedev_label=[]\n",
    "computers_data=[]\n",
    "computers_label=[]\n",
    "'''separate by classes'''\n",
    "for lineNum in range(len(train_data)):\n",
    "    if (train_labels[lineNum]=='rpg'):\n",
    "        rpg_data.append(train_data[lineNum])\n",
    "        rpg_label.append(train_labels[lineNum])\n",
    "    #elif(train_labels[lineNum]=='anime'):\n",
    "    #    anime_data.append(train_data[lineNum])\n",
    "    elif(train_labels[lineNum]=='datascience'):\n",
    "        datascience_data.append(train_data[lineNum])\n",
    "        datascience_label.append(train_labels[lineNum])\n",
    "    elif(train_labels[lineNum]=='hardware'):\n",
    "        hardware_data.append(train_data[lineNum])\n",
    "        hardware_label.append(train_labels[lineNum])\n",
    "    #elif(train_labels[lineNum]=='cars'):\n",
    "    #    cars_data.append(train_data[lineNum])\n",
    "    elif(train_labels[lineNum]=='gamernews'):\n",
    "        gamernews_data.append(train_data[lineNum])\n",
    "        gamernews_label.append(train_labels[lineNum])\n",
    "    elif(train_labels[lineNum]=='gamedev'):\n",
    "        gamedev_data.append(train_data[lineNum])\n",
    "        gamedev_label.append(train_labels[lineNum])\n",
    "    elif(train_labels[lineNum]=='computers'):\n",
    "        computers_data.append(train_data[lineNum])\n",
    "        computers_label.append(train_labels[lineNum])\n",
    "GDvsDS=list.copy(gamedev_data)\n",
    "GDvsDSlabel=list.copy(gamedev_label)\n",
    "for lineNum in range(len(datascience_data)):\n",
    "    GDvsDS.append(datascience_data[lineNum])\n",
    "    GDvsDSlabel.append(datascience_label[lineNum])\n",
    "\n",
    "GNvsHW=list.copy(gamernews_data)\n",
    "GNvsHWlabel=list.copy(gamernews_label)\n",
    "for lineNum in range(len(hardware_data)):\n",
    "    GNvsHW.append(hardware_data[lineNum])\n",
    "    GNvsHWlabel.append(hardware_label[lineNum])\n",
    "    \n",
    "GNvsRPG=list.copy(gamernews_data)\n",
    "GNvsRPGlabel=list.copy(gamernews_label)\n",
    "for lineNum in range(len(gamedev_data)):\n",
    "    GNvsHW.append(rpg_data[lineNum])\n",
    "    GNvsHWlabel.append(rpg_label[lineNum])\n",
    "    \n",
    "PCvsHW=list.copy(computers_data)\n",
    "PCvsHWlabel=list.copy(computers_label)\n",
    "for lineNum in range(len(hardware_data)):\n",
    "    GNvsHW.append(hardware_data[lineNum])\n",
    "    GNvsHWlabel.append(hardware_label[lineNum])\n",
    "\n",
    "'''vectorize each class separately'''\n",
    "def getVocabulary(dataset,datalabel):\n",
    "    vect = TfidfVectorizer(max_df=.9, stop_words=sklearn_stopwords, analyzer='word')\n",
    "    Data=vect.fit_transform(dataset)\n",
    "    features=vect.get_feature_names()\n",
    "    bestData = SelectKBest(chi2,k=1000).fit(Data.toarray(),datalabel)\n",
    "    indexes=bestData.get_support(indices=True)\n",
    "    nfeatures = []\n",
    "    for index in indexes:\n",
    "        nfeatures.append(features[index])\n",
    "    return nfeatures\n",
    "\n",
    "vocabulary=getVocabulary(GNvsHW,GNvsHWlabel)\n",
    "for lineNum in range(len(vocabulary)):\n",
    "  new_features.append(vocabulary[lineNum])\n",
    "vocabulary=getVocabulary(GDvsDS,GDvsDSlabel)\n",
    "for lineNum in range(len(vocabulary)):\n",
    "    new_features.append(vocabulary[lineNum])\n",
    "vocabulary=getVocabulary(GNvsHW,GNvsHWlabel)\n",
    "for lineNum in range(len(vocabulary)):\n",
    "    new_features.append(vocabulary[lineNum])\n",
    "vocabulary=getVocabulary(PCvsHW,PCvsHWlabel)\n",
    "for lineNum in range(len(vocabulary)):\n",
    "    new_features.append(vocabulary[lineNum])\n",
    "#for word in ['ps2','ps3','ps4','ps5','ps1','480p','pass']:\n",
    "#  new_features.append(word)\n",
    "\n",
    "words=set(new_features)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4O_29gSKRFMO"
   },
   "outputs": [],
   "source": [
    "#comment this block out to run logistic regression model\n",
    "\n",
    "#erase before final version\n",
    "pf = pd_train_data.drop_duplicates(subset='body')\n",
    "#print(pd_train_data['body'][9745]==pd_train_data['body'][9785])\n",
    "#print(pf.shape)\n",
    "#train_data = pd_train_data['body'].tolist()\n",
    "#train_labels = pd_train_data['subreddit'].tolist()\n",
    "train_data = pf['body'].tolist()\n",
    "train_labels = pf['subreddit'].tolist()\n",
    "sklearn_stopwords = text.ENGLISH_STOP_WORDS\n",
    "vectorizer = CountVectorizer(strip_accents = ascii, max_features = 10000,binary=True,\n",
    "                             stop_words='english',analyzer='word',ngram_range=(1, 1)) #should we put stop words?\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "train_data2 = X.toarray()\n",
    "train_data = SelectKBest(f_classif,k=5000).fit_transform(train_data2,train_labels)\n",
    "#to do the inverse of the result do not forget to use le object by using inverse_transform(y) method\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_labels = le.fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "q57m-3XGWNpT"
   },
   "outputs": [],
   "source": [
    "'''#uncomment this block to run logistic regression model\n",
    "#vectorize text data\n",
    "vectorizer = TfidfVectorizer(vocabulary=words,analyzer='word',binary=True)  \n",
    "X = vectorizer.fit_transform(train_data)\n",
    "train_data = X.toarray()\n",
    "\n",
    "#to do the inverse of the result do not forget to use le object by using inverse_transform(y) method\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_labels = le.fit_transform(train_labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHWJdV87WUZG",
    "outputId": "30a5cec5-297e-4b16-e8ef-551d9174646f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9105, 5917) (9105,) (2277, 5917) (2277,)\n"
     ]
    }
   ],
   "source": [
    "#create hold out data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20, random_state=42)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjI67JQhWhci"
   },
   "source": [
    "# **Naive** **Bayes** **Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b28ZMXBGWXzY"
   },
   "outputs": [],
   "source": [
    "#create Naivebayes class\n",
    "class Naivebayes:\n",
    "    def fit(self,X,y):\n",
    "            #group training data into distinct classes and store as a list\\n\",\n",
    "            grouped = []\n",
    "            for i in np.unique(y):\n",
    "              temp = []\n",
    "              for j,k in zip(X,y):\n",
    "                if k==i:\n",
    "                  temp.append(j)\n",
    "              grouped.append(temp)\n",
    "            \n",
    "            #calculate prior probabilities of each class and store in a list\n",
    "            #use logs to avoid floating point underflow because we will be multiplying-\n",
    "            #a lot of small numbers together and we dont want them getting approximated to zero\n",
    "                \n",
    "            self.prior_prob = np.array([np.log(len(i)/len(y)) for i in grouped])\n",
    "            print()               \n",
    "               \n",
    "            #calculate conditional probabilities for each class\n",
    "            #variables needed:\n",
    "            #(1) how many documents does word occur for a given class,(2) total no of documents of class and-\n",
    "            #(3) add laplace smoothing constant\n",
    "                \n",
    "            #(1)\n",
    "            word_occurs = np.array([np.array(i).sum(axis=0) for i in grouped])+1\n",
    "        \n",
    "            #(2)\n",
    "            no_of_doc = np.array([[np.array(i).shape[0] for i in grouped]])+2\n",
    "                \n",
    "            #compute conditional probabilities by dividing. Use vectors instead of writing a for loop\n",
    "            self.cond_prob = word_occurs/no_of_doc.T\n",
    "            \n",
    "    def predict(self, X):\n",
    "      #store result of log probabilities\n",
    "      #select the max\n",
    "      #assume X is binary encoded\n",
    "      self.labs = []\n",
    "      for i in X:\n",
    "        result = []\n",
    "        temp_yes = np.log(self.cond_prob)*i #zeros out non occuring words and calculates log of conditional probabilities\n",
    "        temp_no = np.array(1-i)*np.log(1-self.cond_prob) #zeros out occuring words and calculates log of 1-conditional probabilities\n",
    "        temp = temp_yes + temp_no\n",
    "        for j in temp:\n",
    "          result.append(np.array([j]).sum(axis=1))\n",
    "        result = np.array(result).T+self.prior_prob\n",
    "        self.labs.append(np.argmax(result))\n",
    "        \n",
    "    def accu_eval(self,y):\n",
    "      self.acc = 0\n",
    "      for i in range(len(y)):\n",
    "        if self.labs[i]==y[i]:\n",
    "          self.acc+=1\n",
    "      print(\"The accuracy over the test data is\",self.acc/len(y)*100,\"%\")\n",
    "      return self.acc/len(y)\n",
    "              \n",
    "    def write_to_csv(self):\n",
    "      #return csv file with labels\n",
    "      self.labs = le.inverse_transform(self.labs)\n",
    "      dt = {'subreddit':self.labs}\n",
    "      df = pd.DataFrame(dt)\n",
    "      df.to_csv('out.csv',columns=['subreddit'],index_label=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0I0NjAvWgx6",
    "outputId": "b094ce3e-020f-49d4-fd3d-b0b3b7f579c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy over the test data is 87.22002635046113 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8722002635046113"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Naivebayes()\n",
    "d.fit(X_train,y_train)\n",
    "d.predict(X_test)\n",
    "d.accu_eval(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MQ18Wo1NV18",
    "outputId": "351ccf4a-1738-48a2-9607-3ca50e55218e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2898, 5917)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('/content/gdrive/My Drive/ECSE 551 Project 2/test.csv')\n",
    "test_data = test_data['body'].tolist()\n",
    "test_data = vectorizer.transform(test_data)\n",
    "#x_new = SelectKBest(chi2,k=5000).fit(train_data2,train_labels)\n",
    "#test_data = x_new.transform(test_data)\n",
    "test_data = test_data.toarray()\n",
    "print(test_data.shape)\n",
    "\n",
    "td = Naivebayes()\n",
    "td.fit(train_data, train_labels)\n",
    "td.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "nQBqVijSWoe8",
    "outputId": "27137b67-884a-44a0-d70d-d2a872e3ff43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>datascience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>computers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    subreddit\n",
       "0   0  datascience\n",
       "1   1        anime\n",
       "2   2          rpg\n",
       "3   3    computers\n",
       "4   4     hardware"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change filepath\n",
    "td.write_to_csv()\n",
    "s = pd.read_csv('./out.csv')\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_UWPmGJSzij"
   },
   "source": [
    "# **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_NVeJQ-03SrJ"
   },
   "outputs": [],
   "source": [
    "def write_to_csv_sklearn(labs, model):\n",
    "  #return csv file with labels\n",
    "  labs = le.inverse_transform(labs)\n",
    "  dt = {'subreddit': labs}\n",
    "  df = pd.DataFrame(dt)\n",
    "  filename = model + '_out.csv'\n",
    "  df.to_csv(filename, columns=['subreddit'],index_label=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yN_Kygm2e5aL",
    "outputId": "5b3dd704-4ae3-4280-e0c7-48e6a7f8433f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.896354852876592\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "logreg = model.fit(X_train, y_train)\n",
    "\n",
    "predictions = logreg.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print('Logistic Regression Accuracy:', acc)\n",
    "\n",
    "test_predictions = logreg.predict(test_data)\n",
    "\n",
    "write_to_csv_sklearn(test_predictions, 'logreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apevzJySaGNd"
   },
   "source": [
    "# **k**-**fold** **cross** **validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PV751sCmQR8B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#this loop will run n_splits number of times\n",
    "def K_Fold_Eval(n_splits, model, X_train, y_train):\n",
    "  kf = KFold(n_splits, shuffle = True)\n",
    "  accs = []\n",
    "\n",
    "  for train_ind, val_ind in kf.split(X_train):\n",
    "    kf_x_train, kf_x_val = X_train[train_ind], X_train[val_ind]\n",
    "    kf_y_train, kf_y_val = y_train[train_ind], y_train[val_ind]\n",
    "\n",
    "    fitted = model.fit(kf_x_train, kf_y_train)\n",
    "    predictions = fitted.predict(kf_x_val)\n",
    "    acc = accuracy_score(kf_y_val, predictions)\n",
    "    print(acc)\n",
    "    accs.append(acc)\n",
    "  \n",
    "  av_acc = sum(accs)/len(accs)\n",
    "\n",
    "  return av_acc\n",
    "\n",
    "def K_Fold_Eval_NB(n_splits, model, X_train, y_train):\n",
    "  kf = KFold(n_splits = n_splits, shuffle = True)\n",
    "  accs = []\n",
    "\n",
    "  for train_ind, val_ind in kf.split(X_train):\n",
    "    kf_x_train, kf_x_val = X_train[train_ind], X_train[val_ind]\n",
    "    kf_y_train, kf_y_val = y_train[train_ind], y_train[val_ind]\n",
    "\n",
    "    #fitted = model.fit(kf_x_train, kf_y_train)\n",
    "    model.fit(kf_x_train, kf_y_train)\n",
    "    model.predict(kf_x_val)\n",
    "    acc = model.accu_eval(kf_y_val)\n",
    "    accs.append(acc)\n",
    "  \n",
    "  av_acc = sum(accs)/len(accs)\n",
    "\n",
    "  return av_acc\n",
    "\n",
    "\n",
    "#av_acc_log = K_Fold_Eval(10, LogisticRegression(max_iter = 500), X_train, y_train)\n",
    "#print('K-fold Accuracy for Logistic Regression', av_acc_log)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcKvWeYkTEaA",
    "outputId": "552687c1-0255-492a-e53b-3137b01fdb6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n",
      "0.9025460930640913\n",
      "0.9024604569420035\n",
      "0.8804920913884007\n",
      "0.8927943760984183\n",
      "0.9130052724077329\n",
      "0.8884007029876977\n",
      "0.8927943760984183\n",
      "0.8796133567662566\n",
      "0.8945518453427065\n",
      "K-fold Accuracy for Logistic Regression 0.8929011512272196\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression K fold\n",
    "av_acc_log = K_Fold_Eval(10, LogisticRegression(class_weight= None, max_iter = 500), train_data, train_labels)\n",
    "print('K-fold Accuracy for Logistic Regression', av_acc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XdXGP-USaDa",
    "outputId": "6a11c5fa-702d-44a6-ca97-ec93ecce3487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy over the test data is 88.14749780509219 %\n",
      "\n",
      "The accuracy over the test data is 87.00614574187884 %\n",
      "\n",
      "The accuracy over the test data is 87.69771528998243 %\n",
      "\n",
      "The accuracy over the test data is 86.55536028119508 %\n",
      "\n",
      "The accuracy over the test data is 89.103690685413 %\n",
      "\n",
      "The accuracy over the test data is 86.55536028119508 %\n",
      "\n",
      "The accuracy over the test data is 87.17047451669596 %\n",
      "\n",
      "The accuracy over the test data is 87.60984182776801 %\n",
      "\n",
      "The accuracy over the test data is 88.22495606326889 %\n",
      "\n",
      "The accuracy over the test data is 88.92794376098419 %\n",
      "K-fold Accuracy for Naivebayes 0.8769989862534736\n"
     ]
    }
   ],
   "source": [
    "k_fold_NB = K_Fold_Eval_NB(10, Naivebayes(), train_data, train_labels)\n",
    "print('K-fold Accuracy for Naivebayes', k_fold_NB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYv712SunpZ9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "stuff.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
